{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5bbf184e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 指定使用的GPU设备\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"4,5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "61781472",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 17/17 [00:11<00:00,  1.42it/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "# 设置模型检查点路径\n",
    "checkpoint = \"/data/models/huggingface/Qwen/Qwen3-32B\"\n",
    "# 加载分词器和模型\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "# 设置分词器的填充方向为左侧，针对 decoder-only 模型\n",
    "tokenizer.padding_side = 'left'\n",
    "# device_map=\"auto\" 会自动将模型分配到可用的 GPU 上实现模型并行\n",
    "# torch_dtype=\"auto\" 会自动选择合适的精度\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint, torch_dtype=\"auto\", device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "061fd0a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.bfloat16"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# qwen3 模型参数是 fp32, 设置 torch_dtype=\"auto\" 会将其转换为 bf16\n",
    "model.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "798bb7ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[108386,   3837, 107733, 106582,     48,  16948, 101037,  11319]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "prompt = \"你好，你知道什么是Qwen吗？\"\n",
    "# 将提示文本转换为 token_ids\n",
    "# return_tensors=\"pt\" 表示返回 PyTorch 张量格式\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "print(inputs)\n",
    "print(type(inputs[\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e8318802",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[151643, 151643, 151643, 151643, 151643, 151643, 105043, 100165,  11319],\n",
      "        [151643, 151643, 151643, 151643, 151643, 151643, 102762, 106428,  11319],\n",
      "        [107809, 106525, 101883, 101888,     48,  16948, 105427, 101037,  11319]]), 'attention_mask': tensor([[0, 0, 0, 0, 0, 0, 1, 1, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "# tokenizer 也可以直接传入多个提示文本\n",
    "prompts = [\"你是谁？\", \"你会做什么？\", \"你能告诉我一些关于Qwen的信息吗？\"]\n",
    "# 一个 batch 的输入会被填充到相同的长度\n",
    "# padding=True 会自动填充到最长的输入长度\n",
    "# 这样可以确保所有输入的形状一致，便于批处理\n",
    "inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True)\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0b17901e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/easydistill/lib/python3.10/site-packages/transformers/generation/utils.py:2479: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[151643, 151643, 151643, 151643, 151643, 151643, 105043, 100165,  11319,\n",
      "         103929,  71109,  17992, 111558,  26850, 104198,  31935,  64559,  99320,\n",
      "          56007,   3837, 107076, 100338, 111477,  31935,  64559, 104800, 111411,\n",
      "           9370,  71304, 105483, 102064, 104949,   1773,  97611, 105205,  13072,\n",
      "          20412,     48,  16948,   3837, 104811],\n",
      "        [151643, 151643, 151643, 151643, 151643, 151643, 102762, 106428,  11319,\n",
      "            220,    220,  11622, 104811, 102104,    271, 106249, 101951, 102064,\n",
      "         104949,   3837,  35946, 100006, 100364,  20002,  60548, 101312,  88802,\n",
      "           3837, 100630, 116509,  48443,     16,     13,   3070, 102104,  86119,\n",
      "            334,   5122, 102215, 104380,  86119],\n",
      "        [107809, 106525, 101883, 101888,     48,  16948, 105427, 101037,  11319,\n",
      "          84897,  60894,  73670,   6313,  31935,  64559,  99320,  56007,   9909,\n",
      "             48,  16948,   7552, 104625,  31935,  64559, 104800, 100013,   9370,\n",
      "          71304, 105483, 102064, 104949,   1773,  99652, 100006, 102104,  86119,\n",
      "           5373, 104223,  87335,   3837, 101912]])\n",
      "<你是谁？>: 你是谁？你的版本号是多少？\n",
      "\n",
      "我是通义千问，阿里巴巴集团旗下的通义实验室自主研发的超大规模语言模型。我的英文名是Qwen，中文\n",
      "\n",
      "<你会做什么？>: 你会做什么？  用中文回答\n",
      "\n",
      "作为一个大型语言模型，我能够帮助用户完成多种任务，包括但不限于：\n",
      "\n",
      "1. **回答问题**：无论是学术问题\n",
      "\n",
      "<你能告诉我一些关于Qwen的信息吗？>: 你能告诉我一些关于Qwen的信息吗？ 当然可以！通义千问（Qwen）是由通义实验室开发的超大规模语言模型。它能够回答问题、创作文字，比如\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 生成输出\n",
    "# max_new_tokens=64 表示生成的最大新 token 数量\n",
    "outputs = model.generate(**inputs, max_new_tokens=32)\n",
    "print(outputs)\n",
    "\n",
    "# 将生成的 token_ids 转换回文本\n",
    "# skip_special_tokens=True 表示跳过特殊 token，如 <pad>、<eos> 等\n",
    "for i, output in enumerate(outputs):\n",
    "    text = tokenizer.decode(output, skip_special_tokens=True)\n",
    "    print(f\"<{prompts[i]}>: {text}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "easydistill",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
